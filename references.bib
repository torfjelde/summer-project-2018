@article{albanna12_minim_maxim_entrop_distr_binar,
  author =       {Albanna, Badr F. and Hillar, Christopher and
                  Sohl-Dickstein, Jascha and DeWeese, Michael R.},
  title =        {Minimum and Maximum Entropy Distributions for Binary
                  Systems With Known Means and Pairwise Correlations},
  journal =      {CoRR},
  year =         {2012},
  url =          {http://arxiv.org/abs/1209.3744v4},
  abstract =     {Maximum entropy models are increasingly being used
                  to describe the collective activity of neural
                  populations with measured mean neural activities and
                  pairwise correlations, but the full space of
                  probability distributions consistent with these
                  constraints has not been explored. We provide upper
                  and lower bounds on the entropy for the {\em
                  minimum} entropy distribution over arbitrarily large
                  collections of binary units with any fixed set of
                  mean values and pairwise correlations. We also
                  construct specific low-entropy distributions for
                  several relevant cases. Surprisingly, the minimum
                  entropy solution has entropy scaling logarithmically
                  with system size for any set of first- and
                  second-order statistics consistent with arbitrarily
                  large systems. We further demonstrate that some sets
                  of these low-order statistics can only be realized
                  by small systems. Our results show how only small
                  amounts of randomness are needed to mimic low-order
                  statistical properties of highly entropic
                  distributions, and we discuss some applications for
                  engineered and biological information transmission
                  systems.},
  archivePrefix ={arXiv},
  eprint =       {1209.3744},
  primaryClass = {physics.bio-ph},
}

@book{RossSheldonM2014Afci,
  isbn = {9781292024929},
  year = {2014},
  title = {A first course in probability},
  edition = {Ninth edition; Pearson new international edition..},
  language = {eng},
  author = {Ross, Sheldon M},
  keywords = {Probabilities -- Textbooks},
}

@article{paninski_2003,
  author       = {Paninski, Liam},
  title        = {Estimation of Entropy and Mutual Information},
  year         = 2003,
  volume       = 15,
  number       = 6,
  month        = {Jun},
  pages        = {1191–1253},
  issn         = {1530-888X},
  doi          = {10.1162/089976603321780272},
  url          = {http://dx.doi.org/10.1162/089976603321780272},
  journal      = {Neural Computation},
  publisher    = {MIT Press - Journals}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{mehta18_high_bias_low_varian_introd,
  author =       {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao
                  and Day, Alexandre G. R. and Richardson, Clint and
                  Fisher, Charles K. and Schwab, David J.},
  title =        {A High-Bias, Low-Variance Introduction To Machine
                  Learning for Physicists},
  journal =      {CoRR},
  year =         {2018},
  url =          {http://arxiv.org/abs/1803.08823v1},
  abstract =     {Machine Learning (ML) is one of the most exciting
                  and dynamic areas of modern research and
                  application. The purpose of this review is to
                  provide an introduction to the core concepts and
                  tools of machine learning in a manner easily
                  understood and intuitive to physicists. The review
                  begins by covering fundamental concepts in ML and
                  modern statistics such as the bias-variance
                  tradeoff, overfitting, regularization, and
                  generalization before moving on to more advanced
                  topics in both supervised and unsupervised learning.
                  Topics covered in the review include ensemble
                  models, deep learning and neural networks,
                  clustering and data visualization, energy-based
                  models (including MaxEnt models and Restricted
                  Boltzmann Machines), and variational methods.
                  Throughout, we emphasize the many natural
                  connections between ML and statistical physics. A
                  notable aspect of the review is the use of Python
                  notebooks to introduce modern ML/statistical
                  packages to readers using physics-inspired datasets
                  (the Ising Model and Monte-Carlo simulations of
                  supersymmetric decays of proton-proton collisions).
                  We conclude with an extended outlook discussing
                  possible uses of machine learning for furthering our
                  understanding of the physical world as well as open
                  problems in ML where physicists maybe able to
                  contribute. (Notebooks are available at
                  https://physics.bu.edu/~pankajm/MLnotebooks.html )},
  archivePrefix ={arXiv},
  eprint =       {1803.08823},
  primaryClass = {physics.comp-ph},
}

@book{mackay2003information,
  title={Information Theory, Inference and Learning Algorithms},
  author={MacKay, D.J.C. and Kay, D.J.C.M. and Cambridge University Press},
  isbn={9780521642989},
  lccn={2003055133},
  url={https://books.google.co.uk/books?id=AKuMj4PN\_EMC},
  year={2003},
  publisher={Cambridge University Press}
}

@article{shannon1948,
  author       = {Shannon, C. E.},
  title        = {A Mathematical Theory of Communication},
  year         = 1948,
  volume       = 27,
  number       = 3,
  month        = {Jul},
  pages        = {379–423},
  issn         = {0005-8580},
  doi          = {10.1002/j.1538-7305.1948.tb01338.x},
  url          = {http://dx.doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  journal      = {Bell System Technical Journal},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)}
}

@article{jaynes_1957,
  author       = {Jaynes, E. T.},
  title        = {Information Theory and Statistical Mechanics},
  year         = 1957,
  volume       = 106,
  number       = 4,
  month        = {May},
  pages        = {620–630},
  issn         = {0031-899X},
  doi          = {10.1103/physrev.106.620},
  url          = {http://dx.doi.org/10.1103/physrev.106.620},
  journal      = {Physical Review},
  publisher    = {American Physical Society (APS)}
}

@article{guiasu1985,
  author="Guiasu, Silviu and Shenitzer, Abe",
  title="The principle of maximum entropy",
  journal="The Mathematical Intelligencer",
  year="1985",
  month="Mar",
  day="01",
  volume="7",
  number="1",
  pages="42--48",
  issn="0343-6993",
  doi="10.1007/BF03023004",
  url="https://doi.org/10.1007/BF03023004"
}

@article{jaynes1968prior,
  title={Prior probabilities},
  author={Jaynes, Edwin T},
  journal={IEEE Transactions on systems science and cybernetics},
  volume={4},
  number={3},
  pages={227--241},
  year={1968},
  publisher={IEEE}
}

@book{jaynes2003probability,
  title={Probability theory: the logic of science},
  volume={2}
  pages={308-311}
  author={Jaynes, Edwin T},
  year={2003},
  publisher={Cambridge university press}
}

@article{chieu_2002,
  author       = {Chieu, Hai Leong and Ng, Hwee Tou},
  title        = {Named entity recognition},
  year         = 2002,
  doi          = {10.3115/1072228.1072253},
  url          = {http://dx.doi.org/10.3115/1072228.1072253},
  journal      = {Proceedings of the 19th international conference on
                  Computational linguistics -},
  publisher    = {Association for Computational Linguistics}
}

@article{jaynes88,
  author = {Jaynes, Edwin T},
  title = {The Relation of Bayesian and Maximum Entropy Methods},
  year = {1988},
  publisher = {Kluwer Academic Publishers},
  journal = {Maximum-Entropy and Bayesian Methods in Science and Engineering (Vol. 1)},
  url = {https://bayes.wustl.edu/etj/articles/relationship.pdf}
}

@article{berger1996maximum,
  title={A maximum entropy approach to natural language processing},
  author={Berger, Adam L and Pietra, Vincent J Della and Pietra, Stephen A Della},
  journal={Computational linguistics},
  volume={22},
  number={1},
  pages={39--71},
  year={1996},
  publisher={MIT Press}
}

@article{ackley_1985,
  author       = {Ackley, David H. and Hinton, Geoffrey E. and
                  Sejnowski, Terrence J.},
  title        = {A Learning Algorithm for Boltzmann Machines*},
  year         = 1985,
  volume       = 9,
  number       = 1,
  month        = {Jan},
  pages        = {147–169},
  issn         = {0364-0213},
  doi          = {10.1207/s15516709cog0901_7},
  url          = {http://dx.doi.org/10.1207/s15516709cog0901_7},
  journal      = {Cognitive Science},
  publisher    = {Wiley}
}

@article{endres_2003,
  author       = {Endres, D.M. and Schindelin, J.E.},
  title        = {A new metric for probability distributions},
  year         = 2003,
  volume       = 49,
  number       = 7,
  month        = {Jul},
  pages        = {1858–1860},
  issn         = {0018-9448},
  doi          = {10.1109/tit.2003.813506},
  url          = {http://dx.doi.org/10.1109/tit.2003.813506},
  journal      = {IEEE Transactions on Information Theory},
  publisher    = {Institute of Electrical and Electronics Engineers
                  (IEEE)}
}

@book{rbm_intro_smolensky,
  author={David E. Rumelhart, James L. McClelland and the PDP Research Group, Institute for Cognitive Science, University of California, San Diego.},
  city={Cambridge, Ma.},
  ed={11th print.},
  form={[BC]},
  isbn={[0-262-68053-X]},
  lang={eng},
  lccn={[85024073 e02000073]},
  oclcnum={[40638324 508286432 60863132 612201300 61307352 613502778 62230114 664598721 764240305 781712345 856734454 868610433 247774667 248384591 248613402 60445750 631611931 633114512 634243646 715828319 716357310 718013980 769236326 12837549 179701872 464791029 611984182 613572278 615327440 800843359 863548415]},
  publisher={MIT Press},
  title={Parallel distributed processing : explorations in the microstructure of cognition.},
  url={[http://www.worldcat.org/oclc/40638324?referer=xid]},
  year={1986},
  pages={194-263}
}

@article{Fischer_2015,
  author       = {Fischer, Asja},
  title        = {Training Restricted Boltzmann Machines},
  keywords =     {summer_project_2018},
  year         = 2015,
  volume       = 29,
  number       = 4,
  month        = {May},
  pages        = {441–444},
  issn         = {1610-1987},
  doi          = {10.1007/s13218-015-0371-2},
  url          = {http://dx.doi.org/10.1007/s13218-015-0371-2},
  journal      = {KI - Künstliche Intelligenz},
  publisher    = {Springer Nature}
}

@incollection{hinton2012practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, Geoffrey E},
  booktitle={Neural networks: Tricks of the trade},
  pages={599--619},
  year={2012},
  publisher={Springer}
}

@article{neal98_anneal_impor_sampl,
  author =       {Neal, Radford M.},
  title =        {Annealed Importance Sampling},
  journal =      {CoRR},
  year =         {1998},
  url =          {http://arxiv.org/abs/physics/9803008v2},
  abstract =     {Simulated annealing - moving from a tractable
                  distribution to a distribution of interest via a
                  sequence of intermediate distributions - has
                  traditionally been used as an inexact method of
                  handling isolated modes in Markov chain samplers.
                  Here, it is shown how one can use the Markov chain
                  transitions for such an annealing sequence to define
                  an importance sampler. The Markov chain aspect
                  allows this method to perform acceptably even for
                  high-dimensional problems, where finding good
                  importance sampling distributions would otherwise be
                  very difficult, while the use of importance weights
                  ensures that the estimates found converge to the
                  correct values as the number of annealing runs
                  increases. This annealed importance sampling
                  procedure resembles the second half of the
                  previously-studied tempered transitions, and can be
                  seen as a generalization of a recently-proposed
                  variant of sequential importance sampling. It is
                  also related to thermodynamic integration methods
                  for estimating ratios of normalizing constants.
                  Annealed importance sampling is most attractive when
                  isolated modes are present, or when estimates of
                  normalizing constants are required, but it may also
                  be more generally useful, since its independent
                  sampling allows one to bypass some of the problems
                  of assessing convergence and autocorrelation in
                  Markov chain samplers.},
  archivePrefix ={arXiv},
  eprint =       {physics/9803008},
  primaryClass = {physics.comp-ph},
}


@article{tak16_repel_attrac_metrop_algor_multim,
  author =       {Tak, Hyungsuk and Meng, Xiao-Li and Dyk, David A.
                  van},
  title =        {A Repelling-Attracting Metropolis Algorithm for
                  Multimodality},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1601.05633v5},
  abstract =     {Although the Metropolis algorithm is simple to
                  implement, it often has difficulties exploring
                  multimodal distributions. We propose the
                  repelling-attracting Metropolis (RAM) algorithm that
                  maintains the simple-to-implement nature of the
                  Metropolis algorithm, but is more likely to jump
                  between modes. The RAM algorithm is a
                  Metropolis-Hastings algorithm with a proposal that
                  consists of a downhill move in density that aims to
                  make local modes repelling, followed by an uphill
                  move in density that aims to make local modes
                  attracting. The downhill move is achieved via a
                  reciprocal Metropolis ratio so that the algorithm
                  prefers downward movement. The uphill move does the
                  opposite using the standard Metropolis ratio which
                  prefers upward movement. This down-up movement in
                  density increases the probability of a proposed move
                  to a different mode. Because the acceptance
                  probability of the proposal involves a ratio of
                  intractable integrals, we introduce an auxiliary
                  variable which creates a term in the acceptance
                  probability that cancels with the intractable ratio.
                  Using several examples, we demonstrate the potential
                  for the RAM algorithm to explore a multimodal
                  distribution more efficiently than a Metropolis
                  algorithm and with less tuning than is commonly
                  required by tempering-based methods.},
  archivePrefix ={arXiv},
  eprint =       {1601.05633},
  primaryClass = {stat.ME},
}

@article{lan13_wormh_hamil_monte_carlo,
  author =       {Lan, Shiwei and Streets, Jeffrey and Shahbaba,
                  Babak},
  title =        {Wormhole Hamiltonian Monte Carlo},
  journal =      {CoRR},
  year =         2013,
  url =          {http://arxiv.org/abs/1306.0063v2},
  abstract =     {In machine learning and statistics, probabilistic
                  inference involving multimodal distributions is
                  quite difficult. This is especially true in high
                  dimensional problems, where most existing algorithms
                  cannot easily move from one mode to another. To
                  address this issue, we propose a novel Bayesian
                  inference approach based on Markov Chain Monte
                  Carlo. Our method can effectively sample from
                  multimodal distributions, especially when the
                  dimension is high and the modes are isolated. To
                  this end, it exploits and modifies the Riemannian
                  geometric properties of the target distribution to
                  create \emph{wormholes} connecting modes in order to
                  facilitate moving between them. Further, our
                  proposed method uses the regeneration technique in
                  order to adapt the algorithm by identifying new
                  modes and updating the network of wormholes without
                  affecting the stationary distribution. To find new
                  modes, as opposed to rediscovering those previously
                  identified, we employ a novel mode searching
                  algorithm that explores a \emph{residual energy}
                  function obtained by subtracting an approximate
                  Gaussian mixture density (based on previously
                  discovered modes) from the target density function.},
  archivePrefix ={arXiv},
  eprint =       {1306.0063},
  primaryClass = {stat.CO},
}

