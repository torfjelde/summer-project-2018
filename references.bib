@article{albanna12_minim_maxim_entrop_distr_binar,
  author =       {Albanna, Badr F. and Hillar, Christopher and
                  Sohl-Dickstein, Jascha and DeWeese, Michael R.},
  title =        {Minimum and Maximum Entropy Distributions for Binary
                  Systems With Known Means and Pairwise Correlations},
  journal =      {CoRR},
  year =         {2012},
  url =          {http://arxiv.org/abs/1209.3744v4},
  abstract =     {Maximum entropy models are increasingly being used
                  to describe the collective activity of neural
                  populations with measured mean neural activities and
                  pairwise correlations, but the full space of
                  probability distributions consistent with these
                  constraints has not been explored. We provide upper
                  and lower bounds on the entropy for the {\em
                  minimum} entropy distribution over arbitrarily large
                  collections of binary units with any fixed set of
                  mean values and pairwise correlations. We also
                  construct specific low-entropy distributions for
                  several relevant cases. Surprisingly, the minimum
                  entropy solution has entropy scaling logarithmically
                  with system size for any set of first- and
                  second-order statistics consistent with arbitrarily
                  large systems. We further demonstrate that some sets
                  of these low-order statistics can only be realized
                  by small systems. Our results show how only small
                  amounts of randomness are needed to mimic low-order
                  statistical properties of highly entropic
                  distributions, and we discuss some applications for
                  engineered and biological information transmission
                  systems.},
  archivePrefix ={arXiv},
  eprint =       {1209.3744},
  primaryClass = {physics.bio-ph},
}

@book{RossSheldonM2014Afci,
  isbn = {9781292024929},
  year = {2014},
  title = {A first course in probability},
  edition = {Ninth edition; Pearson new international edition..},
  language = {eng},
  author = {Ross, Sheldon M},
  keywords = {Probabilities -- Textbooks},
}

@article{paninski_2003,
  author       = {Paninski, Liam},
  title        = {Estimation of Entropy and Mutual Information},
  year         = 2003,
  volume       = 15,
  number       = 6,
  month        = {Jun},
  pages        = {1191–1253},
  issn         = {1530-888X},
  doi          = {10.1162/089976603321780272},
  url          = {http://dx.doi.org/10.1162/089976603321780272},
  journal      = {Neural Computation},
  publisher    = {MIT Press - Journals}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{mehta18_high_bias_low_varian_introd,
  author =       {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao
                  and Day, Alexandre G. R. and Richardson, Clint and
                  Fisher, Charles K. and Schwab, David J.},
  title =        {A High-Bias, Low-Variance Introduction To Machine
                  Learning for Physicists},
  journal =      {CoRR},
  year =         {2018},
  url =          {http://arxiv.org/abs/1803.08823v1},
  abstract =     {Machine Learning (ML) is one of the most exciting
                  and dynamic areas of modern research and
                  application. The purpose of this review is to
                  provide an introduction to the core concepts and
                  tools of machine learning in a manner easily
                  understood and intuitive to physicists. The review
                  begins by covering fundamental concepts in ML and
                  modern statistics such as the bias-variance
                  tradeoff, overfitting, regularization, and
                  generalization before moving on to more advanced
                  topics in both supervised and unsupervised learning.
                  Topics covered in the review include ensemble
                  models, deep learning and neural networks,
                  clustering and data visualization, energy-based
                  models (including MaxEnt models and Restricted
                  Boltzmann Machines), and variational methods.
                  Throughout, we emphasize the many natural
                  connections between ML and statistical physics. A
                  notable aspect of the review is the use of Python
                  notebooks to introduce modern ML/statistical
                  packages to readers using physics-inspired datasets
                  (the Ising Model and Monte-Carlo simulations of
                  supersymmetric decays of proton-proton collisions).
                  We conclude with an extended outlook discussing
                  possible uses of machine learning for furthering our
                  understanding of the physical world as well as open
                  problems in ML where physicists maybe able to
                  contribute. (Notebooks are available at
                  https://physics.bu.edu/~pankajm/MLnotebooks.html )},
  archivePrefix ={arXiv},
  eprint =       {1803.08823},
  primaryClass = {physics.comp-ph},
}

@book{mackay2003information,
  title={Information Theory, Inference and Learning Algorithms},
  author={MacKay, D.J.C. and Kay, D.J.C.M. and Cambridge University Press},
  isbn={9780521642989},
  lccn={2003055133},
  url={https://books.google.co.uk/books?id=AKuMj4PN\_EMC},
  year={2003},
  publisher={Cambridge University Press}
}

@article{shannon1948,
  author       = {Shannon, C. E.},
  title        = {A Mathematical Theory of Communication},
  year         = 1948,
  volume       = 27,
  number       = 3,
  month        = {Jul},
  pages        = {379–423},
  issn         = {0005-8580},
  doi          = {10.1002/j.1538-7305.1948.tb01338.x},
  url          = {http://dx.doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  journal      = {Bell System Technical Journal},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)}
}

@article{jaynes_1957,
  author       = {Jaynes, E. T.},
  title        = {Information Theory and Statistical Mechanics},
  year         = 1957,
  volume       = 106,
  number       = 4,
  month        = {May},
  pages        = {620–630},
  issn         = {0031-899X},
  doi          = {10.1103/physrev.106.620},
  url          = {http://dx.doi.org/10.1103/physrev.106.620},
  journal      = {Physical Review},
  publisher    = {American Physical Society (APS)}
}

@article{guiasu1985,
  author="Guiasu, Silviu and Shenitzer, Abe",
  title="The principle of maximum entropy",
  journal="The Mathematical Intelligencer",
  year="1985",
  month="Mar",
  day="01",
  volume="7",
  number="1",
  pages="42--48",
  issn="0343-6993",
  doi="10.1007/BF03023004",
  url="https://doi.org/10.1007/BF03023004"
}

@article{jaynes1968prior,
  title={Prior probabilities},
  author={Jaynes, Edwin T},
  journal={IEEE Transactions on systems science and cybernetics},
  volume={4},
  number={3},
  pages={227--241},
  year={1968},
  publisher={IEEE}
}

@book{jaynes2003probability,
  title={Probability theory: the logic of science},
  volume={2}
  pages={308-311}
  author={Jaynes, Edwin T},
  year={2003},
  publisher={Cambridge university press}
}

@article{chieu_2002,
  author       = {Chieu, Hai Leong and Ng, Hwee Tou},
  title        = {Named entity recognition},
  year         = 2002,
  doi          = {10.3115/1072228.1072253},
  url          = {http://dx.doi.org/10.3115/1072228.1072253},
  journal      = {Proceedings of the 19th international conference on
                  Computational linguistics -},
  publisher    = {Association for Computational Linguistics}
}

@article{jaynes88,
  author = {Jaynes, Edwin T},
  title = {The Relation of Bayesian and Maximum Entropy Methods},
  year = {1988},
  publisher = {Kluwer Academic Publishers},
  journal = {Maximum-Entropy and Bayesian Methods in Science and Engineering (Vol. 1)},
  url = {https://bayes.wustl.edu/etj/articles/relationship.pdf}
}

@article{berger1996maximum,
  title={A maximum entropy approach to natural language processing},
  author={Berger, Adam L and Pietra, Vincent J Della and Pietra, Stephen A Della},
  journal={Computational linguistics},
  volume={22},
  number={1},
  pages={39--71},
  year={1996},
  publisher={MIT Press}
}

@article{ackley_1985,
  author       = {Ackley, David H. and Hinton, Geoffrey E. and
                  Sejnowski, Terrence J.},
  title        = {A Learning Algorithm for Boltzmann Machines*},
  year         = 1985,
  volume       = 9,
  number       = 1,
  month        = {Jan},
  pages        = {147–169},
  issn         = {0364-0213},
  doi          = {10.1207/s15516709cog0901_7},
  url          = {http://dx.doi.org/10.1207/s15516709cog0901_7},
  journal      = {Cognitive Science},
  publisher    = {Wiley}
}