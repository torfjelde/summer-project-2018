{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "LOG_LEVEL = \"INFO\"\n",
    "LOG_FORMAT = '%(asctime)-15s %(levelname)-9s %(name)s: %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, LOG_LEVEL))\n",
    "\n",
    "log = logging.getLogger(\"rbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines (RBMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_0 = X_train[0]\n",
    "h_0 = rbm.sample_hidden(v_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1 = v_0 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.repeat(v_0.reshape(1, -1), 3, axis=0)\n",
    "h = np.repeat(h_0.reshape(1, -1), 3, axis=0)\n",
    "\n",
    "v[0] = v[0] + 1\n",
    "v[1] = v[1] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(v, rbm.c) + np.matmul(h, rbm.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(h, rbm.W.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.matmul(h, rbm.W.T)\n",
    "x[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(h, np.matmul(v, rbm.W).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(np.matmul(np.matmul(h, rbm.W.T), v.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.dot(rbm.c, v_0) + np.dot(rbm.b, h_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(v_0, np.dot(rbm.W, h_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.dot(rbm.c, v_1) + np.dot(rbm.b, h_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(v_1, np.dot(rbm.W, h_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.matmul(v, rbm.c) +\n",
    " np.matmul(h, rbm.b) +\n",
    " np.diag(np.matmul(np.matmul(h, rbm.W.T), v.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2 = BernoulliRBM(784, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.b.shape, np.matmul(v, rbm2.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.proba_hidden(v).shape == h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.proba_visible(h).shape == v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.sample_hidden(v).shape == h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.sample_visible(h).shape == v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v__0, v_k = rbm2.contrastive_divergence(v, k=4)\n",
    "dc, db, dw = rbm2.grad(v__0, v_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.shape, db.shape, dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.step(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rbm.free_energy(x) for x in X_train[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.free_energy(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.c = rbm2.c\n",
    "rbm.b = rbm2.b\n",
    "rbm.W = rbm2.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train[:5])):\n",
    "    a = [rbm.proba_hidden(x) for x in X_train[:5]][i]\n",
    "    b = rbm2.proba_hidden(X_train[:5])[i]\n",
    "    eps = 0.000001\n",
    "\n",
    "    assert np.all((b + eps > a) & (b - eps < a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients\n",
    "v = X_train[:5]\n",
    "v_k = rbm2.sample_visible(rbm2.sample_hidden(X_train[:5]))\n",
    "\n",
    "a = rbm2.grad(v, v_k)\n",
    "b = [rbm.grad(v[i], v_k[i]) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [np.array(x) for x in list(zip(*b))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.all(np.all((b[i] + eps > a[i]) & (b[i] - eps < a[i]))) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm2.free_energy(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([rbm.free_energy(x) for x in X_train[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = np.matmul(v, rbm2.c)\n",
    "hidden = rbm2.b + np.matmul(v, rbm2.W)\n",
    "- (visible + np.sum(np.log(1 + np.exp(hidden)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class BatchBernoulliRBM(object):\n",
    "        \"\"\"\n",
    "        RBM with Bernoulli variables for hidden and visible states.\n",
    "        \"\"\"\n",
    "        def __init__(self, num_visible, num_hidden):\n",
    "            super(BatchBernoulliRBM, self).__init__()\n",
    "            self.num_visible = num_visible\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "            self.c, self.b, self.W = self.initialize(self.num_visible, self.num_hidden)\n",
    "\n",
    "        @staticmethod\n",
    "        def initialize(num_visible, num_hidden):\n",
    "            # biases for visible and hidden, respectively\n",
    "            c = np.zeros(num_visible)\n",
    "            b = np.zeros(num_hidden)\n",
    "\n",
    "            # weight matrix\n",
    "            W = np.random.normal(0.0, 0.01, (num_visible, num_hidden))\n",
    "\n",
    "            return c, b, W\n",
    "\n",
    "        def energy(self, v, h):\n",
    "            return - (np.matmul(v, self.c) +\n",
    "                      np.matmul(h, self.b) +\n",
    "                      np.matmul(h, np.matmul(v, rbm.W).T))\n",
    "\n",
    "        def proba_visible(self, h):\n",
    "            \"Computes p(v | h).\"\n",
    "            return sigmoid(self.c + np.matmul(h, self.W.T))\n",
    "\n",
    "        def proba_hidden(self, v):\n",
    "            \"Computes p(h | h).\"\n",
    "            return sigmoid(self.b + np.matmul(v, self.W))\n",
    "\n",
    "        def sample_visible(self, h):\n",
    "            \"Samples visible units from the given hidden units `h`.\"\n",
    "            # compute p(V_j = 1 | h)\n",
    "            probas = self.proba_visible(h)\n",
    "            # equiv. of V_j ~ p(V_j | h)\n",
    "            rands = np.random.random(size=probas.shape)\n",
    "            v = (probas > rands).astype(int)\n",
    "            return v\n",
    "\n",
    "        def sample_hidden(self, v):\n",
    "            \"Samples hidden units from the given visible units `v`.\"\n",
    "            # compute p(H_{\\mu} = 1 | v)\n",
    "            probas = self.proba_hidden(v)\n",
    "            # euqiv. of H_{\\mu} ~ p(H_{\\mu} | h)\n",
    "            rands = np.random.random(size=probas.shape)\n",
    "            h = (probas > rands).astype(np.int)\n",
    "            return h\n",
    "\n",
    "        def free_energy(self, v):\n",
    "            # unnormalized\n",
    "            # F(v) = - log \\tilde{p}(v) = - \\log \\sum_{h} \\exp ( - E(v, h))\n",
    "            # using Eq. 2.20 (Fischer, 2015) for \\tilde{p}(v)\n",
    "            if len(v.shape) < 2:\n",
    "                v = v.reshape(1, -1)\n",
    "            visible = np.matmul(v, self.c)\n",
    "            hidden = self.b + np.matmul(v, self.W)\n",
    "            return - (visible + np.sum(np.log(1 + np.exp(hidden)), axis=1))\n",
    "\n",
    "        def contrastive_divergence(self, v_0, k=1):\n",
    "            v = v_0\n",
    "            for t in range(k):\n",
    "                h = self.sample_hidden(v)\n",
    "                v = self.sample_visible(h)\n",
    "\n",
    "            return v_0, v\n",
    "\n",
    "        def grad(self, v_0, v_k):\n",
    "            \"Estimates the gradient of the negative log-likelihood using CD-k.\"\n",
    "            proba_h_0 = self.proba_hidden(v_0)\n",
    "            proba_h_k = self.proba_hidden(v_k)\n",
    "\n",
    "            delta_c = v_0 - v_k\n",
    "            delta_b = proba_h_0 - proba_h_k\n",
    "\n",
    "            x = proba_h_0.reshape(proba_h_0.shape[0], 1, proba_h_0.shape[1])\n",
    "            y = v_0.reshape(v_0.shape[0], v_0.shape[1], 1)\n",
    "            z_0 = np.matmul(y, x)\n",
    "\n",
    "            x = proba_h_k.reshape(proba_h_k.shape[0], 1, proba_h_k.shape[1])\n",
    "            y = v_k.reshape(v_k.shape[0], v_k.shape[1], 1)\n",
    "            z_k = np.matmul(y, x)\n",
    "            delta_W = z_0 - z_k\n",
    "\n",
    "            return delta_c, delta_b, delta_W\n",
    "\n",
    "        def step(self, vs, k=1, lr=0.1, lmda=0.0):\n",
    "            \"Performs a single gradient ascent step using CD-k on the batch `vs`.\"\n",
    "            v_0, v_k = self.contrastive_divergence(v, k=k)\n",
    "\n",
    "            # compute gradient for each observed visible configuration\n",
    "            dc, db, dW = self.grad(v_0, v_k)\n",
    "\n",
    "            # accumulate gradients\n",
    "            delta_c = np.mean(dc, axis=0)\n",
    "            delta_b = np.mean(db, axis=0)\n",
    "            delta_W = np.mean(dW, axis=0)\n",
    "\n",
    "            # update parameters\n",
    "            self.c += lr * delta_c\n",
    "            self.b += lr * delta_b\n",
    "            self.W += lr * delta_W\n",
    "\n",
    "            # possible apply weight-decay\n",
    "            if lmda > 0.0:\n",
    "                self.c -= lmda * self.c\n",
    "                self.b -= lmda * self.b\n",
    "                self.W -= lmda * self.W\n",
    "\n",
    "        def fit(self, train_data, k=1, learning_rate=0.01, num_epochs=5, batch_size=64, test_data=None):\n",
    "            num_samples = train_data.shape[0]\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            loglikelihood_train = []\n",
    "            loglikelihood = []\n",
    "\n",
    "            for epoch in range(1, num_epochs + 1):\n",
    "                # compute train & test negative log-likelihood\n",
    "                nll_train = np.mean(rbm.free_energy(train_data))\n",
    "                # nll_train = np.mean(np.array([rbm.free_energy(v) for v in train_data]))\n",
    "                loglikelihood_train.append(nll_train)\n",
    "                log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (train): {nll_train:>20.5f}\")\n",
    "\n",
    "                if test_data is not None:\n",
    "                    nll = np.mean(np.array([rbm.free_energy(v) for v in test_data]))\n",
    "                    log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (test):  {nll:>20.5f}\")\n",
    "                    loglikelihood.append(nll)\n",
    "\n",
    "                # iterate through dataset in batches\n",
    "                bar = tqdm(total=num_samples)\n",
    "                for start in range(0, num_samples, batch_size):\n",
    "                    # ensure we don't go out-of-bounds\n",
    "                    end = min(start + batch_size, num_samples)\n",
    "\n",
    "                    # take a gradient-step\n",
    "                    rbm.step(train_data[start: end], k=k, lr=learning_rate)\n",
    "\n",
    "                    # update progress\n",
    "                    bar.update(end - start)\n",
    "\n",
    "                bar.close()\n",
    "\n",
    "                # shuffle indices for next epoch\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "            # compute train & test negative log-likelihood of final batch\n",
    "            nll_train = np.mean([rbm.free_energy(v) for v in train_data])\n",
    "            loglikelihood_train.append(nll_train)\n",
    "            log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (train): {nll_train:>20.5f}\")\n",
    "\n",
    "            if test_data is not None:\n",
    "                nll = np.mean([rbm.free_energy(v) for v in test_data])\n",
    "                log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (test):  {nll:>20.5f}\")\n",
    "                loglikelihood.append(nll)\n",
    "\n",
    "            return loglikelihood_train, loglikelihood\n",
    "\n",
    "        def loss(self, samples_true, per_sample_hidden=100):\n",
    "            \"\"\"\n",
    "            Computes the difference in empirical distributions observed in `samples_true` and samples\n",
    "            of visible units obtained from the model, using the same initial state.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            samples_true: array-like\n",
    "                True samples of visible units to compare to.\n",
    "            per_sample_hidden: int\n",
    "                Number of `h` to \"partially marginalize\" over when computing p(v) = p(v | h) p(h).\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            loss: float\n",
    "                Difference between the two distributions.\n",
    "\n",
    "            Notes\n",
    "            -----\n",
    "            Can be very inaccurate estimate of how well the model is performing since one is NOT completely\n",
    "            marginalizing out all hidden variables.\n",
    "\n",
    "            \"\"\"\n",
    "            k = per_sample_hidden\n",
    "            # the loss is the log energy-difference between the p(v) and p(v_k), where `v_k` is the Gibbs sampled visible unit\n",
    "            return np.mean([\n",
    "                self.free_energy(v) - self.free_energy(self.contrastive_divergence(v, k)[1])\n",
    "                for v in samples_true\n",
    "            ])\n",
    "\n",
    "        def reconstruct(self, v, num_samples=100):\n",
    "            samples = self.sample_visible(self.sample_hidden(v))\n",
    "            for _ in range(num_samples - 1):\n",
    "                samples += self.sample_visible(self.sample_hidden(v))\n",
    "\n",
    "            probs = samples / num_samples \n",
    "\n",
    "            return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # clip the values due to possibility of overflow\n",
    "    return 1.0 / (1.0 + np.exp(-np.maximum(np.minimum(z, 30), -30)))\n",
    "\n",
    "\n",
    "### Restricted Boltzmann Machine ###\n",
    "class BernoulliRBM(object):\n",
    "    \"\"\"\n",
    "    RBM with Bernoulli variables for hidden and visible states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        super(BernoulliRBM, self).__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self.c, self.b, self.W = self.initialize(self.num_visible, self.num_hidden)\n",
    "      \n",
    "    @staticmethod\n",
    "    def initialize(num_visible, num_hidden):\n",
    "        # biases for visible and hidden, respectively\n",
    "        c = np.zeros(num_visible)\n",
    "        b = np.zeros(num_hidden)\n",
    "\n",
    "        # weight matrix\n",
    "        # Results on MNIST are highly dependent on this initialization\n",
    "        W = np.random.normal(0.0, 0.01, (num_visible, num_hidden))\n",
    "        \n",
    "        return c, b, W\n",
    "\n",
    "    def energy(self, v, h):\n",
    "        return - (np.dot(self.c, v) + np.dot(self.b, h) + np.dot(v, np.dot(self.W, h)))\n",
    "\n",
    "    def proba_visible(self, h):\n",
    "        \"Computes p(v | h).\"\n",
    "        return sigmoid(self.c + np.dot(self.W, h))\n",
    "\n",
    "    def proba_hidden(self, v):\n",
    "        \"Computes p(h | h).\"\n",
    "        return sigmoid(self.b + np.dot(self.W.T, v))\n",
    "\n",
    "    def sample_visible(self, h):\n",
    "        \"Samples visible units from the given hidden units `h`.\"\n",
    "        # compute p(V_j = 1 | h)\n",
    "        probas = self.proba_visible(h)\n",
    "        # equiv. of V_j ~ p(V_j | h)\n",
    "        rands = np.random.random(size=probas.shape)\n",
    "        v = (probas > rands).astype(int)\n",
    "        return v\n",
    "\n",
    "    def sample_hidden(self, v):\n",
    "        \"Samples hidden units from the given visible units `v`.\"\n",
    "        # compute p(H_{\\mu} = 1 | v)\n",
    "        probas = self.proba_hidden(v)\n",
    "        # euqiv. of H_{\\mu} ~ p(H_{\\mu} | h)\n",
    "        rands = np.random.random(size=probas.shape)\n",
    "        h = (probas > rands).astype(np.int)\n",
    "        return h\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        # unnormalized\n",
    "        # F(v) = - log \\tilde{p}(v) = - \\log \\sum_{h} \\exp ( - E(v, h))\n",
    "        # using Eq. 2.20 (Fischer, 2015) for \\tilde{p}(v)\n",
    "        visible = np.dot(self.c, v)\n",
    "        hidden = self.b + np.dot(self.W.T, v)\n",
    "        return - (visible + np.sum(np.log(1 + np.exp(hidden))))\n",
    "    \n",
    "    def contrastive_divergence(self, v_0, k=1):\n",
    "        h = self.sample_hidden(v_0)\n",
    "        v = self.sample_visible(h)\n",
    "        \n",
    "        if k > 1:\n",
    "            for t in range(k):\n",
    "                h = self.sample_hidden(v)\n",
    "                v = self.sample_visible(h)\n",
    "                \n",
    "        return v_0, v\n",
    "\n",
    "    def grad(self, v_0, v_k):\n",
    "        \"Estimates the gradient of the negative log-likelihood using CD-k.\"\n",
    "        proba_h_0 = self.proba_hidden(v_0)\n",
    "        proba_h_k = self.proba_hidden(v_k)\n",
    "        \n",
    "        delta_c = v_0 - v_k\n",
    "        delta_b = proba_h_0 - proba_h_k\n",
    "\n",
    "        # reshape so that we can compute v_j h_{\\mu} by\n",
    "        # taking the dot product to obtain `delta_W`\n",
    "        v_0 = np.reshape(v_0, (-1, 1))\n",
    "        proba_h_0 = np.reshape(proba_h_0, (1, -1))\n",
    "        \n",
    "        v_k = np.reshape(v_k, (-1, 1))\n",
    "        proba_h_k = np.reshape(proba_h_k, (1, -1))\n",
    "        \n",
    "        delta_W = np.dot(v_0, proba_h_0) - np.dot(v_k, proba_h_k)\n",
    "        \n",
    "        return delta_c, delta_b, delta_W\n",
    "    \n",
    "    def step(self, vs, k=1, lr=0.1, lmda=0.0):\n",
    "        \"Performs a single gradient ascent step using CD-k on the batch `vs`.\"\n",
    "        # TODO: can we perform this over the batch using matrix multiplication instead?\n",
    "        v_0, v_k = self.contrastive_divergence(vs[0], k=k)\n",
    "        delta_c, delta_b, delta_W = self.grad(v_0, v_k)\n",
    "        for v in vs[1:]:\n",
    "            # perform CD-k\n",
    "            v_0, v_k = self.contrastive_divergence(v, k=k)\n",
    "            # compute gradient for each observed visible configuration\n",
    "            dc, db, dW = self.grad(v_0, v_k)\n",
    "            # accumulate gradients\n",
    "            delta_c += dc\n",
    "            delta_b += db\n",
    "            delta_W += dW\n",
    "\n",
    "        # update parameters\n",
    "        self.c += lr * (delta_c / len(vs))\n",
    "        self.b += lr * (delta_b / len(vs))\n",
    "        self.W += lr * (delta_W / len(vs))\n",
    "\n",
    "        # possible apply weight-decay\n",
    "        if lmda > 0.0:\n",
    "            self.c -= lmda * self.c\n",
    "            self.b -= lmda * self.b\n",
    "            self.W -= lmda * self.W\n",
    "        \n",
    "    def loss(self, samples_true, per_sample_hidden=100):\n",
    "        \"\"\"\n",
    "        Computes the difference in free energy, i.e.\n",
    "        \n",
    "            log p(v_k) - log p(v_0)\n",
    "            \n",
    "        where `v_0` is sample from data, and `v_k` is the sample from the chain initialized at v_0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples_true: array-like\n",
    "            True samples of visible units to compare to.\n",
    "        per_sample_hidden: int\n",
    "            Number of `h` to \"partially marginalize\" over when computing p(v) = p(v | h) p(h).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            Difference between the two distributions.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Could also consider the reconstruction error, i.e. how much the mean-squared error between the\n",
    "        observed v_0 and the generated v_k\n",
    "\n",
    "        \"\"\"\n",
    "        # the loss is the log energy-difference between the p(v) and p(v_k), where `v_k` is the Gibbs sampled visible unit\n",
    "        return np.mean([\n",
    "            self.free_energy(v) - self.free_energy(self.contrastive_divergence(v, per_sample_hidden)[1])\n",
    "            for v in samples_true\n",
    "        ])\n",
    "\n",
    "    def reconstruct(self, v, num_samples=100):\n",
    "        samples = self.sample_visible(self.sample_hidden(v))\n",
    "        for _ in range(num_samples - 1):\n",
    "            samples += self.sample_visible(self.sample_hidden(v))\n",
    "\n",
    "        probs = samples / num_samples \n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly making sure everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testah\n",
    "num_visible = 100\n",
    "num_hidden = 50\n",
    "rbm = BernoulliRBM(num_visible, num_hidden)\n",
    "# v = np.random.randint(2, size=num_visible)\n",
    "v = np.random.randint(2, size=num_visible)\n",
    "h = np.random.randint(2, size=num_hidden)\n",
    "rbm.energy(v, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = rbm.sample_hidden(v)\n",
    "v = rbm.sample_visible(h)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_0, v_k = rbm.contrastive_divergence(v, k=10)\n",
    "v_0, v_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc, db, dW = rbm.grad(v_0, v_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.shape, db.shape, dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.W.max(), rbm.c.max(), rbm.b.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Alternative method to load MNIST, since mldata.org is often down...\n",
    "from scipy.io import loadmat\n",
    "mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "mnist_path = \"./mnist-original.mat\"\n",
    "\n",
    "if os.path.exists(mnist_path):\n",
    "    log.info(f\"Found existing file at {mnist_path}; loading...\")\n",
    "    mnist_raw = loadmat(mnist_path)\n",
    "    mnist = {\n",
    "        \"data\": mnist_raw[\"data\"].T,\n",
    "        \"target\": mnist_raw[\"label\"][0],\n",
    "        \"COL_NAMES\": [\"label\", \"data\"],\n",
    "        \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "    }\n",
    "else:\n",
    "    log.info(f\"Dataset not found at {mnist_path}; downloading...\")\n",
    "    response = urllib.request.urlopen(mnist_alternative_url)\n",
    "    with open(mnist_path, \"wb\") as f:\n",
    "        content = response.read()\n",
    "        f.write(content)\n",
    "    mnist_raw = loadmat(mnist_path)\n",
    "    mnist = {\n",
    "        \"data\": mnist_raw[\"data\"].T,\n",
    "        \"target\": mnist_raw[\"label\"][0],\n",
    "        \"COL_NAMES\": [\"label\", \"data\"],\n",
    "        \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "    }\n",
    "    log.info(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# in case we want to use `cupy` to run on the GPU\n",
    "X = np.asarray(mnist[\"data\"])\n",
    "y = np.asarray(mnist[\"target\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# clip values since we're working with binary variables and original images have domain [0, 255]\n",
    "X_train = X_train.clip(0, 1)\n",
    "X_test = X_test.clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    import cupy as np\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "NUM_HIDDEN = 500\n",
    "\n",
    "# model\n",
    "rbm = BernoulliRBM(X_train.shape[1], NUM_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.W.shape, rbm.c.shape, rbm.b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.1\n",
    "K = 1\n",
    "\n",
    "# train\n",
    "log.info(f\"Starting training\")\n",
    "num_samples = X_train.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "loglikelihood = []\n",
    "loglikelihood_train = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # compute train & test negative log-likelihood\n",
    "    nll_train = np.mean([rbm.free_energy(v) for v in X_train])\n",
    "    loglikelihood_train.append(nll_train)\n",
    "    log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (train): {nll_train:>20.5f}\")\n",
    "    \n",
    "    nll = np.mean([rbm.free_energy(v) for v in X_test])\n",
    "    log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (test):  {nll:>20.5f}\")\n",
    "    loglikelihood.append(nll)\n",
    "\n",
    "    # iterate through dataset in batches\n",
    "    bar = tqdm(total=num_samples)\n",
    "    for start in range(0, num_samples, BATCH_SIZE):\n",
    "        # ensure we don't go out-of-bounds\n",
    "        end = min(start + BATCH_SIZE, num_samples)\n",
    "        \n",
    "        # take a gradient-step\n",
    "        rbm.step(X_train[start: end], k=K, lr=LEARNING_RATE)\n",
    "        \n",
    "        # update progress\n",
    "        bar.update(end - start)\n",
    "        \n",
    "    bar.close()\n",
    "\n",
    "    # shuffle indices for next epoch\n",
    "    np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(- np.array(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display reconstructions of test-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(x, p):\n",
    "    x = x.copy()\n",
    "    mask = np.random.random(size=x.shape) < p\n",
    "    flipped = (~(x.astype(np.bool))).astype(np.int)\n",
    "    x[mask] = flipped[mask]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a couple of testing examples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot some reconstructions\n",
    "from matplotlib import gridspec\n",
    "\n",
    "n_rows = 6\n",
    "n_cols = 8\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, \n",
    "                         sharex=True, sharey=True, \n",
    "                         figsize=(16, 12),\n",
    "                         # make it tight\n",
    "                         gridspec_kw=dict(wspace=-0.1, hspace=-0.01))\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols // 2):\n",
    "        v_0 = X_test[np.random.randint(X_test.shape[0])]\n",
    "        \n",
    "        # introduce noise\n",
    "        noise = 0.1\n",
    "        v = flip(v_0, noise)\n",
    "        \n",
    "        probs = rbm.reconstruct(v, num_samples=1000)\n",
    "#         num_samples = 100\n",
    "#         k = 100\n",
    "#         _, probs = rbm.contrastive_divergence(v, k=k)\n",
    "\n",
    "#         for _ in range(num_samples - 1):\n",
    "#             probs += rbm.contrastive_divergence(v, k=k)[1]\n",
    "        \n",
    "#         probs = probs / num_samples\n",
    "\n",
    "        # in case we've substituted with `cupy`\n",
    "        if np.__name__ != \"numpy\":\n",
    "            v = np.asnumpy(v)\n",
    "            probs = np.asnumpy(probs)\n",
    "            \n",
    "        log.info(f\"Reconstruction error of {(i, 2 * j)}: {np.mean(np.abs(v_0 - probs))} (noise: {noise})\")\n",
    "\n",
    "        axes[i][2 * j].imshow(np.reshape(v, (28, 28)))\n",
    "        axes[i][2 * j + 1].imshow(np.reshape(probs, (28, 28)))\n",
    "\n",
    "        # customization; remove labels\n",
    "        axes[i][2 * j].set_xticklabels([])\n",
    "        axes[i][2 * j].set_yticklabels([])\n",
    "\n",
    "        axes[i][2 * j + 1].set_xticklabels([])\n",
    "        axes[i][2 * j + 1].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_updown_couplings(N, coupling_values=[0, 1]):\n",
    "    \"Returns an of dim N x N x N x N, representing 2D neighbor-couplings for each entry.\"\n",
    "    J = np.zeros((N, N, N, N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i + 1 < N:\n",
    "                J[i, j, i + 1, j] = np.random.choice(coupling_values)\n",
    "            if i - 1 > 0:\n",
    "                J[i, j, i - 1, j] = np.random.choice(coupling_values)\n",
    "            if j + 1 < N:\n",
    "                J[i, j, i, j + 1] = np.random.choice(coupling_values)\n",
    "            if j - 1 > 0:\n",
    "                J[i, j, i, j - 1] = np.random.choice(coupling_values)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Ising(object):\n",
    "    \"\"\"\n",
    "    Ising model.\n",
    "    \"\"\"\n",
    "#     boltzmann_constant = 1.38064852e-23\n",
    "    \n",
    "    def __init__(self, T, N, field=None, couplings=None, domain=[0, 1]):\n",
    "        super(Ising, self).__init__()\n",
    "        self.domain = domain\n",
    "        self.N = N\n",
    "#         self.beta = 1.0 / (self.boltzmann_constant * T)\n",
    "        self.beta = 1.0 / T\n",
    "        self.T = T\n",
    "        \n",
    "        self.h = field if field is not None else np.random.randn(N, N)\n",
    "        self.J = couplings if couplings is not None else create_updown_couplings(N, self.domain)\n",
    "\n",
    "    def energy(self, x):\n",
    "        # (J[s] * x) gives us J_{s, i} * x_i for all i, and then we multiply by \n",
    "        couplings_energy = np.sum([\n",
    "            np.sum(self.J[i, j] * x * x[i, j])\n",
    "            for i in range(self.N)\n",
    "            for j in range(self.N)])\n",
    "        field_energy = np.sum(self.h * x)\n",
    "        return - (field_energy + 0.5 * couplings_energy)\n",
    "    \n",
    "\n",
    "    def sample(self, state, full=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n: int\n",
    "            Number of timesteps to sample for\n",
    "\n",
    "        \"\"\"\n",
    "        if full:\n",
    "            indices = it.product(range(N), range(N))\n",
    "        else:\n",
    "            indices = [np.random.randint(self.N, size=2)]\n",
    "        \n",
    "        for (i, j) in indices:\n",
    "            delta_E = np.sum(self.J[i, j] * state)\n",
    "    #         delta_E = 2 * (delta_E + self.h[i, j])\n",
    "            delta_E = (delta_E + self.h[i, j])\n",
    "\n",
    "            u = np.random.uniform()\n",
    "            new_state = state.copy()\n",
    "\n",
    "            if delta_E > 0:\n",
    "                if np.exp(- self.beta * delta_E) > u:\n",
    "                    # accept flip\n",
    "                    new_state[i, j] = self.domain[-1] if state[i, j] < self.domain[-1] else self.domain[0]\n",
    "            else:\n",
    "                # flip\n",
    "                new_state[i, j] = self.domain[-1] if state[i, j] < self.domain[-1] else self.domain[0]\n",
    "                \n",
    "            # update\n",
    "            state = new_state\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "N = 10\n",
    "\n",
    "domain = [0, 1]\n",
    "couplings = create_updown_couplings(N, coupling_values=domain)\n",
    "couplings[couplings != 0.0] = 1.0\n",
    "\n",
    "# state = np.random.choice(domain, size=(N, N))\n",
    "state = np.zeros((N, N)) + 1.0\n",
    "state[N // 2, N // 2] = domain[0]\n",
    "state[N // 2, N // 2 + 1] = domain[0]\n",
    "\n",
    "ising = Ising(33, N, field=np.zeros((N, N)), couplings=couplings)\n",
    "energy = ising.energy(state)\n",
    "energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = state\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burn-in\n",
    "for _ in range(N * N):\n",
    "    x = ising.sample(x)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    x = ising.sample(x)\n",
    "    history.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# burn-in\n",
    "for _ in range(N * N):\n",
    "    x = ising.sample(x)\n",
    "\n",
    "while True:\n",
    "    plt.imshow(x)\n",
    "    for _ in range(N):\n",
    "        x = ising.sample(x)\n",
    "        history.append(x)\n",
    "#     x = ising.sample(x)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a RBM on an Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.array([np.reshape(x, -1) for x in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    import cupy as np\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "NUM_HIDDEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(num_visible, NUM_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.1\n",
    "K = 1\n",
    "\n",
    "# train\n",
    "log.info(f\"Starting training\")\n",
    "num_samples = X_train.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "loglikelihood = []\n",
    "loglikelihood_train = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # # Might be a good idea to increase the `k` as we get nearer to the end, so that our gradient estimate\n",
    "    # # is more accurate\n",
    "    # if epoch == NUM_EPOCHS:\n",
    "    #     log.info(f\"[{epoch} / {NUM_EPOCHS}] Increasing k: {K} -> {5 * K}\")\n",
    "    #     K = 5 * K\n",
    "    # else:\n",
    "    nll_train = np.mean([rbm.free_energy(v) for v in X_train])\n",
    "    loglikelihood_train.append(nll_train)\n",
    "    log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (train): {nll_train:>20.5f}\")\n",
    "    \n",
    "    nll = np.mean([rbm.free_energy(v) for v in X_test])\n",
    "    log.info(f\"[{epoch} / {NUM_EPOCHS}] NLL (test):  {nll:>20.5f}\")\n",
    "    loglikelihood.append(nll)\n",
    "\n",
    "    bar = tqdm(total=num_samples)\n",
    "    for start in range(0, num_samples, BATCH_SIZE):\n",
    "        # ensure we don't go out-of-bounds\n",
    "        end = min(start + BATCH_SIZE, num_samples)\n",
    "        \n",
    "        # take a gradient-step\n",
    "        rbm.step(X_train[start: end], k=K, lr=LEARNING_RATE)\n",
    "        \n",
    "        # update progress\n",
    "        bar.update(end - start)\n",
    "    bar.close()\n",
    "\n",
    "    # shuffle indices for next epoch\n",
    "    np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(rbm.b).max(), np.abs(rbm.c).max(), np.abs(rbm.W).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from Ising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 16))\n",
    "\n",
    "v = X_test[np.random.randint(len(X_test))]\n",
    "v_k = rbm.reconstruct(np.reshape(v, -1))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    axes[0].imshow(np.reshape(v, (N, N)))\n",
    "    axes[1].imshow(np.reshape(v_k, (N, N)))\n",
    "\n",
    "    v = ising.sample(np.reshape(v, (N, N)))\n",
    "    \n",
    "    # Reconstruct from `v` if you don't want the entire chain to be sampled\n",
    "    # by the RBM, but only the next step\n",
    "#     v_k = rbm.sample_visible(rbm.sample_hidden(np.reshape(v, -1)))\n",
    "    v_k = rbm.reconstruct(np.reshape(v, -1))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
