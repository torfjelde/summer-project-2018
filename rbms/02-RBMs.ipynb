{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "LOG_LEVEL = \"INFO\"\n",
    "LOG_FORMAT = '%(asctime)-15s %(levelname)-9s %(name)s: %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, LOG_LEVEL))\n",
    "\n",
    "log = logging.getLogger(\"rbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines (RBMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # clip the values due to possibility of overflow\n",
    "    return 1.0 / (1.0 + np.exp(-np.maximum(np.minimum(z, 30), -30)))\n",
    "\n",
    "\n",
    "### Restricted Boltzmann Machine ###\n",
    "class BernoulliRBM(object):\n",
    "    \"\"\"\n",
    "    RBM with Bernoulli variables for hidden and visible states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        super(BernoulliRBM, self).__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self.c, self.b, self.W = self.initialize(self.num_visible, self.num_hidden)\n",
    "      \n",
    "    @staticmethod\n",
    "    def initialize(num_visible, num_hidden):\n",
    "        # biases for visible and hidden, respectively\n",
    "        c = np.zeros(num_visible)\n",
    "        b = np.zeros(num_hidden)\n",
    "\n",
    "        # weight matrix\n",
    "        # Results on MNIST are highly dependent on this initialization\n",
    "        W = np.random.normal(0.0, 1.0, (num_visible, num_hidden))\n",
    "        \n",
    "        return c, b, W\n",
    "\n",
    "    def energy(self, v, h):\n",
    "        return - (np.dot(self.c, v) + np.dot(self.b, h) + np.dot(v, np.dot(self.W, h)))\n",
    "\n",
    "    def proba_visible(self, h):\n",
    "        \"Computes p(v | h).\"\n",
    "        return sigmoid(self.c + np.dot(self.W, h))\n",
    "\n",
    "    def proba_hidden(self, v):\n",
    "        \"Computes p(h | h).\"\n",
    "        return sigmoid(self.b + np.dot(self.W.T, v))\n",
    "\n",
    "    def sample_visible(self, h):\n",
    "        \"Samples visible units from the given hidden units `h`.\"\n",
    "        # compute p(V_j = 1 | h)\n",
    "        probas = self.proba_visible(h)\n",
    "        # equiv. of V_j ~ p(V_j | h)\n",
    "        rands = np.random.random(size=probas.shape)\n",
    "        v = (probas > rands).astype(int)\n",
    "        return v\n",
    "\n",
    "    def sample_hidden(self, v):\n",
    "        \"Samples hidden units from the given visible units `v`.\"\n",
    "        # compute p(H_{\\mu} = 1 | v)\n",
    "        probas = self.proba_hidden(v)\n",
    "        # euqiv. of H_{\\mu} ~ p(H_{\\mu} | h)\n",
    "        rands = np.random.random(size=probas.shape)\n",
    "        h = (probas > rands).astype(np.int)\n",
    "        return h\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        # unnormalized\n",
    "        # F(v) = - log \\tilde{p}(v) = - \\log \\sum_{h} \\exp ( - E(v, h))\n",
    "        # using Eq. 2.20 (Fischer, 2015) for \\tilde{p}(v)\n",
    "        visible = np.dot(self.c, v)\n",
    "        hidden = self.b + np.dot(self.W.T, v)\n",
    "        return - (visible + np.sum(np.log(1 + np.exp(hidden))))\n",
    "    \n",
    "    def contrastive_divergence(self, v_0, k=1):\n",
    "        h = self.sample_hidden(v_0)\n",
    "        v = self.sample_visible(h)\n",
    "        \n",
    "        if k > 1:\n",
    "            for t in range(k):\n",
    "                h = self.sample_hidden(v)\n",
    "                v = self.sample_visible(h)\n",
    "                \n",
    "        return v_0, v\n",
    "\n",
    "    def grad(self, v_0, v_k):\n",
    "        \"Estimates the gradient of the negative log-likelihood using CD-k.\"\n",
    "        proba_h_0 = self.proba_hidden(v_0)\n",
    "        proba_h_k = self.proba_hidden(v_k)\n",
    "        \n",
    "        delta_c = v_0 - v_k\n",
    "        delta_b = proba_h_0 - proba_h_k\n",
    "\n",
    "        # reshape so that we can compute v_j h_{\\mu} by\n",
    "        # taking the dot product to obtain `delta_W`\n",
    "        v_0 = np.reshape(v_0, (-1, 1))\n",
    "        proba_h_0 = np.reshape(proba_h_0, (1, -1))\n",
    "        \n",
    "        v_k = np.reshape(v_k, (-1, 1))\n",
    "        proba_h_k = np.reshape(proba_h_k, (1, -1))\n",
    "        \n",
    "        delta_W = np.dot(v_0, proba_h_0) - np.dot(v_k, proba_h_k)\n",
    "        \n",
    "        return delta_c, delta_b, delta_W\n",
    "    \n",
    "    def step(self, vs, k=1, lr=0.1, lmda=0.0):\n",
    "        \"Performs a single gradient ascent step using CD-k on the batch `vs`.\"\n",
    "        # TODO: can we perform this over the batch using matrix multiplication instead?\n",
    "        v_0, v_k = self.contrastive_divergence(vs[0], k=k)\n",
    "        delta_c, delta_b, delta_W = self.grad(v_0, v_k)\n",
    "        for v in vs[1:]:\n",
    "            # perform CD-k\n",
    "            v_0, v_k = self.contrastive_divergence(v, k=k)\n",
    "            # compute gradient for each observed visible configuration\n",
    "            dc, db, dW = self.grad(v_0, v_k)\n",
    "            # accumulate gradients\n",
    "            delta_c += dc\n",
    "            delta_b += db\n",
    "            delta_W += dW\n",
    "\n",
    "        # update parameters\n",
    "        self.c += lr * (delta_c / len(vs))\n",
    "        self.b += lr * (delta_b / len(vs))\n",
    "        self.W += lr * (delta_W / len(vs))\n",
    "\n",
    "        # possible apply weight-decay\n",
    "        if lmda > 0.0:\n",
    "            self.c -= lmda * self.c\n",
    "            self.b -= lmda * self.b\n",
    "            self.W -= lmda * self.W\n",
    "        \n",
    "    def loss(self, samples_true, per_sample_hidden=100):\n",
    "        \"\"\"\n",
    "        Computes the difference in free energy, i.e.\n",
    "        \n",
    "            log p(v_k) - log p(v_0)\n",
    "            \n",
    "        where `v_0` is sample from data, and `v_k` is the sample from the chain initialized at v_0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples_true: array-like\n",
    "            True samples of visible units to compare to.\n",
    "        per_sample_hidden: int\n",
    "            Number of `h` to \"partially marginalize\" over when computing p(v) = p(v | h) p(h).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            Difference between the two distributions.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Could also consider the reconstruction error, i.e. how much the mean-squared error between the\n",
    "        observed v_0 and the generated v_k\n",
    "\n",
    "        \"\"\"\n",
    "        # the loss is the log energy-difference between the p(v) and p(v_k), where `v_k` is the Gibbs sampled visible unit\n",
    "        return np.mean([\n",
    "            self.free_energy(v) - self.free_energy(self.contrastive_divergence(v, per_sample_hidden)[1])\n",
    "            for v in samples_true\n",
    "        ])\n",
    "\n",
    "    def reconstruct(self, v, num_samples=100):\n",
    "        samples = self.sample_visible(self.sample_hidden(v))\n",
    "        for _ in range(num_samples - 1):\n",
    "            samples += self.sample_visible(self.sample_hidden(v))\n",
    "\n",
    "        probs = samples / num_samples \n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly making sure everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testah\n",
    "num_visible = 100\n",
    "num_hidden = 50\n",
    "rbm = BernoulliRBM(num_visible, num_hidden)\n",
    "# v = np.random.randint(2, size=num_visible)\n",
    "v = np.random.randint(2, size=num_visible)\n",
    "h = np.random.randint(2, size=num_hidden)\n",
    "rbm.energy(v, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = rbm.sample_hidden(v)\n",
    "v = rbm.sample_visible(h)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_0, v_k = rbm.contrastive_divergence(v, k=10)\n",
    "v_0, v_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc, db, dW = rbm.grad(v_0, v_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.shape, db.shape, dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.W.max(), rbm.c.max(), rbm.b.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Alternative method to load MNIST, since mldata.org is often down...\n",
    "from scipy.io import loadmat\n",
    "mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "mnist_path = \"./mnist-original.mat\"\n",
    "\n",
    "if os.path.exists(mnist_path):\n",
    "    log.info(f\"Found existing file at {mnist_path}; loading...\")\n",
    "    mnist_raw = loadmat(mnist_path)\n",
    "    mnist = {\n",
    "        \"data\": mnist_raw[\"data\"].T,\n",
    "        \"target\": mnist_raw[\"label\"][0],\n",
    "        \"COL_NAMES\": [\"label\", \"data\"],\n",
    "        \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "    }\n",
    "else:\n",
    "    log.info(f\"Dataset not found at {mnist_path}; downloading...\")\n",
    "    response = urllib.request.urlopen(mnist_alternative_url)\n",
    "    with open(mnist_path, \"wb\") as f:\n",
    "        content = response.read()\n",
    "        f.write(content)\n",
    "    mnist_raw = loadmat(mnist_path)\n",
    "    mnist = {\n",
    "        \"data\": mnist_raw[\"data\"].T,\n",
    "        \"target\": mnist_raw[\"label\"][0],\n",
    "        \"COL_NAMES\": [\"label\", \"data\"],\n",
    "        \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "    }\n",
    "    log.info(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# in case we want to use `cupy` to run on the GPU\n",
    "X = np.asarray(mnist[\"data\"])\n",
    "y = np.asarray(mnist[\"target\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# clip values since we're working with binary variables and original images have domain [0, 255]\n",
    "X_train = X_train.clip(0, 1)\n",
    "X_test = X_test.clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    import cupy as np\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "NUM_HIDDEN = 500\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "K = 1\n",
    "\n",
    "# model\n",
    "rbm = BernoulliRBM(X_train.shape[1], NUM_HIDDEN)\n",
    "\n",
    "# train\n",
    "log.info(f\"Starting training\")\n",
    "num_samples = X_train.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # # Might be a good idea to increase the `k` as we get nearer to the end, so that our gradient estimate\n",
    "    # # is more accurate\n",
    "    # if epoch == NUM_EPOCHS:\n",
    "    #     log.info(f\"[{epoch} / {NUM_EPOCHS}] Increasing k: {K} -> {5 * K}\")\n",
    "    #     K = 5 * K\n",
    "    # else:\n",
    "    log.info(f\"[{epoch} / {NUM_EPOCHS}]\")\n",
    "\n",
    "    bar = tqdm(total=num_samples)\n",
    "    for start in range(0, num_samples, BATCH_SIZE):\n",
    "        # ensure we don't go out-of-bounds\n",
    "        end = min(start + BATCH_SIZE, num_samples)\n",
    "        \n",
    "        # take a gradient-step\n",
    "        rbm.step(X_train[start: end], k=K, lr=LEARNING_RATE)\n",
    "        \n",
    "        # update progress\n",
    "        bar.update(end - start)\n",
    "\n",
    "    # shuffle indices for next epoch\n",
    "    np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display reconstructions of test-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a couple of testing examples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot some reconstructions\n",
    "from matplotlib import gridspec\n",
    "\n",
    "n_rows = 6\n",
    "n_cols = 8\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, \n",
    "                         sharex=True, sharey=True, \n",
    "                         figsize=(16, 12),\n",
    "                         # make it tight\n",
    "                         gridspec_kw=dict(wspace=-0.1, hspace=-0.01))\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols // 2):\n",
    "        v = X_test[np.random.randint(X_test.shape[0])]\n",
    "        probs = rbm.reconstruct(v)\n",
    "\n",
    "        # in case we've substituted with `cupy`\n",
    "        if np.__name__ != \"numpy\":\n",
    "            v = np.asnumpy(v)\n",
    "            probs = np.asnumpy(probs)\n",
    "\n",
    "        axes[i][2 * j].imshow(np.reshape(v, (28, 28)))\n",
    "        axes[i][2 * j + 1].imshow(np.reshape(probs, (28, 28)))\n",
    "\n",
    "        # customization; remove labels\n",
    "        axes[i][2 * j].set_xticklabels([])\n",
    "        axes[i][2 * j].set_yticklabels([])\n",
    "\n",
    "        axes[i][2 * j + 1].set_xticklabels([])\n",
    "        axes[i][2 * j + 1].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_updown_couplings(N, coupling_values=[0, 1]):\n",
    "    \"Returns an of dim N x N x N x N, representing 2D neighbor-couplings for each entry.\"\n",
    "    J = np.zeros((N, N, N, N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i + 1 < N:\n",
    "                J[i, j, i + 1, j] = np.random.choice(coupling_values)\n",
    "            if i - 1 > 0:\n",
    "                J[i, j, i - 1, j] = np.random.choice(coupling_values)\n",
    "            if j + 1 < N:\n",
    "                J[i, j, i, j + 1] = np.random.choice(coupling_values)\n",
    "            if j - 1 > 0:\n",
    "                J[i, j, i, j - 1] = np.random.choice(coupling_values)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Ising(object):\n",
    "    \"\"\"\n",
    "    Ising model.\n",
    "    \"\"\"\n",
    "#     boltzmann_constant = 1.38064852e-23\n",
    "    \n",
    "    def __init__(self, T, N, field=None, couplings=None, domain=[0, 1]):\n",
    "        super(Ising, self).__init__()\n",
    "        self.domain = domain\n",
    "        self.N = N\n",
    "#         self.beta = 1.0 / (self.boltzmann_constant * T)\n",
    "        self.beta = 1.0 / T\n",
    "        self.T = T\n",
    "        \n",
    "        self.h = field if field is not None else np.random.randn(N, N)\n",
    "        self.J = couplings if couplings is not None else create_updown_couplings(N, self.domain)\n",
    "\n",
    "    def energy(self, x):\n",
    "        # (J[s] * x) gives us J_{s, i} * x_i for all i, and then we multiply by \n",
    "        couplings_energy = np.sum([\n",
    "            np.sum(self.J[i, j] * x * x[i, j])\n",
    "            for i in range(self.N)\n",
    "            for j in range(self.N)])\n",
    "        field_energy = np.sum(self.h * x)\n",
    "        return - (field_energy + 0.5 * couplings_energy)\n",
    "    \n",
    "\n",
    "    def sample(self, state, full=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n: int\n",
    "            Number of timesteps to sample for\n",
    "\n",
    "        \"\"\"\n",
    "        if full:\n",
    "            indices = it.product(range(N), range(N))\n",
    "        else:\n",
    "            indices = [np.random.randint(self.N, size=2)]\n",
    "        \n",
    "        for (i, j) in indices:\n",
    "            delta_E = np.sum(self.J[i, j] * state)\n",
    "    #         delta_E = 2 * (delta_E + self.h[i, j])\n",
    "            delta_E = (delta_E + self.h[i, j])\n",
    "\n",
    "            u = np.random.uniform()\n",
    "            new_state = state.copy()\n",
    "\n",
    "            if delta_E > 0:\n",
    "                if np.exp(- self.beta * delta_E) > u:\n",
    "                    # accept flip\n",
    "                    new_state[i, j] = self.domain[-1] if state[i, j] < self.domain[-1] else self.domain[0]\n",
    "            else:\n",
    "                # flip\n",
    "                new_state[i, j] = self.domain[-1] if state[i, j] < self.domain[-1] else self.domain[0]\n",
    "                \n",
    "            # update\n",
    "            state = new_state\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "N = 10\n",
    "\n",
    "domain = [0, 1]\n",
    "couplings = create_updown_couplings(N, coupling_values=domain)\n",
    "couplings[couplings != 0.0] = 1.0\n",
    "\n",
    "# state = np.random.choice(domain, size=(N, N))\n",
    "state = np.zeros((N, N)) + 1.0\n",
    "state[N // 2, N // 2] = domain[0]\n",
    "state[N // 2, N // 2 + 1] = domain[0]\n",
    "\n",
    "ising = Ising(33, N, field=np.zeros((N, N)), couplings=couplings)\n",
    "energy = ising.energy(state)\n",
    "energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = state\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burn-in\n",
    "for _ in range(N * N):\n",
    "    x = ising.sample(x)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    x = ising.sample(x)\n",
    "    history.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# burn-in\n",
    "for _ in range(N * N):\n",
    "    x = ising.sample(x)\n",
    "\n",
    "while True:\n",
    "    plt.imshow(x)\n",
    "    for _ in range(N):\n",
    "        x = ising.sample(x)\n",
    "        history.append(x)\n",
    "#     x = ising.sample(x)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a RBM on an Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.array([np.reshape(x, -1) for x in history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    import cupy as np\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "NUM_HIDDEN = 500\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(num_visible, NUM_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(X_train)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    log.info(f\"[{epoch} / {NUM_EPOCHS}]\")\n",
    "\n",
    "    bar = tqdm(total=num_samples)\n",
    "    for start in range(0, num_samples, BATCH_SIZE):\n",
    "        # ensure we don't go out-of-bounds\n",
    "        end = min(start + BATCH_SIZE, num_samples)\n",
    "        \n",
    "        # take a gradient-step\n",
    "        rbm.step(X_train[start: end], k=K, lr=LEARNING_RATE)\n",
    "        \n",
    "        # update progress\n",
    "        bar.update(end - start)\n",
    "\n",
    "    # shuffle indices for next epoch\n",
    "    np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.b.max(), rbm.c.max(), rbm.W.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from Ising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 16))\n",
    "\n",
    "v = X_test[np.random.randint(len(X_test))]\n",
    "\n",
    "while True:\n",
    "    v_k = rbm.reconstruct(np.reshape(v, -1))\n",
    "    axes[0].imshow(np.reshape(v, (N, N)))\n",
    "    axes[1].imshow(np.reshape(v_k, (N, N)))\n",
    "\n",
    "    v = ising.sample(np.reshape(v, (N, N)))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
